import feedparser
import google.generativeai as genai
import os
import json
import requests
import urllib.parse
from wordcloud import WordCloud
from janome.tokenizer import Tokenizer

# --- è¨­å®šã‚¨ãƒªã‚¢ --------------------------
# æ¤œç´¢ã—ãŸã„ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’ã“ã“ã«è¿½åŠ ã—ã¾ã™
KEYWORDS = [
    "AI", 
    "äººå·¥çŸ¥èƒ½", 
    "æ©Ÿæ¢°å­¦ç¿’", 
    "ãƒ‡ã‚£ãƒ¼ãƒ—ãƒ©ãƒ¼ãƒ‹ãƒ³ã‚°", 
    "ãƒ•ã‚£ã‚¸ã‚«ãƒ«AI",
    "ç”»åƒèªè­˜",
    "ç”ŸæˆAI",
    "LLM",
    "ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«AI",
    "AGI"
]

# å–å¾—ã™ã‚‹è¨˜äº‹æ•° (5 -> 10ã«å¤‰æ›´)
MAX_ARTICLES = 10

# ãƒ¯ãƒ¼ãƒ‰ã‚¯ãƒ©ã‚¦ãƒ‰ç”¨æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆã®è¨­å®š
FONT_URL = "https://github.com/googlefonts/noto-cjk/raw/main/Sans/OTF/Japanese/NotoSansCJKjp-Regular.otf"
FONT_PATH = "NotoSansCJKjp-Regular.otf"
# ----------------------------------------

def download_font():
    """ãƒ¯ãƒ¼ãƒ‰ã‚¯ãƒ©ã‚¦ãƒ‰ç”¨ã®æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã™ã‚‹"""
    if not os.path.exists(FONT_PATH):
        print("ğŸ”¤ æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ä¸­...")
        try:
            response = requests.get(FONT_URL, timeout=30)
            with open(FONT_PATH, 'wb') as f:
                f.write(response.content)
        except Exception as e:
            print(f"ãƒ•ã‚©ãƒ³ãƒˆãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã‚¨ãƒ©ãƒ¼: {e}")

def create_wordcloud(text_list):
    """ãƒ‹ãƒ¥ãƒ¼ã‚¹ã®ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰ãƒ¯ãƒ¼ãƒ‰ã‚¯ãƒ©ã‚¦ãƒ‰ç”»åƒã‚’ç”Ÿæˆã™ã‚‹"""
    print("â˜ï¸ ãƒ¯ãƒ¼ãƒ‰ã‚¯ãƒ©ã‚¦ãƒ‰ã‚’ç”Ÿæˆä¸­...")
    
    # 1. ãƒ†ã‚­ã‚¹ãƒˆã‚’çµåˆ
    full_text = " ".join(text_list)
    
    # 2. å½¢æ…‹ç´ è§£æã§åè©ã ã‘æŠ½å‡º (Janomeä½¿ç”¨)
    t = Tokenizer()
    tokens = t.tokenize(full_text)
    words = []
    
    # é™¤å¤–ã—ãŸã„ä¸€èˆ¬çš„ãªå˜èªï¼ˆã‚¹ãƒˆãƒƒãƒ—ãƒ¯ãƒ¼ãƒ‰ï¼‰
    stop_words = ["ã“ã¨", "ã‚‚ã®", "ãŸã‚", "ã‚ˆã†", "ãã‚Œ", "ã“ã‚Œ", "ã•ã‚“", "ã®", "ã‚“", "AI", "æ´»ç”¨", "å¯¾å¿œ", "é–‹ç™º", "ç™ºè¡¨", "æä¾›", "æ©Ÿèƒ½", "ã‚µãƒ¼ãƒ“ã‚¹", "æŠ€è¡“", "åˆ©ç”¨", "æ—¥æœ¬", "ä¼æ¥­"]
    
    for token in tokens:
        # åè©ã®ã¿æŠ½å‡ºã—ã€ã‹ã¤ã‚¹ãƒˆãƒƒãƒ—ãƒ¯ãƒ¼ãƒ‰ã«å«ã¾ã‚Œãªã„ã‚‚ã®
        if token.part_of_speech.split(',')[0] == 'åè©' and token.surface not in stop_words:
            words.append(token.surface)
    
    if not words:
        print("ãƒ¯ãƒ¼ãƒ‰ã‚¯ãƒ©ã‚¦ãƒ‰ç”Ÿæˆç”¨ã®å˜èªãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
        return None

    # 3. ã‚¹ãƒšãƒ¼ã‚¹åŒºåˆ‡ã‚Šã®æ–‡å­—åˆ—ã«ã™ã‚‹
    text_space = " ".join(words)
    
    # 4. ãƒ•ã‚©ãƒ³ãƒˆã®æº–å‚™
    download_font()
    
    # 5. ç”»åƒç”Ÿæˆ
    try:
        wc = WordCloud(
            font_path=FONT_PATH if os.path.exists(FONT_PATH) else None, # ãƒ•ã‚©ãƒ³ãƒˆæŒ‡å®š
            width=800, 
            height=400, 
            background_color='white',
            colormap='viridis', # è‰²ä½¿ã„
            regexp=r"[\w']+"    # æ—¥æœ¬èªå¯¾å¿œã®ãŸã‚ã®æ­£è¦è¡¨ç¾
        )
        wc.generate(text_space)
        
        # ç”»åƒã‚’ä¿å­˜
        output_filename = "wordcloud.png"
        wc.to_file(output_filename)
        return output_filename
    except Exception as e:
        print(f"ãƒ¯ãƒ¼ãƒ‰ã‚¯ãƒ©ã‚¦ãƒ‰ç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}")
        return None

def get_rss_url():
    """è¨­å®šã—ãŸã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã¨æœŸé–“(1æ—¥ä»¥å†…)ã‹ã‚‰æ¤œç´¢ç”¨URLã‚’ä½œæˆã™ã‚‹"""
    query_string = " OR ".join(KEYWORDS)
    final_query = f"({query_string}) when:1d"
    encoded_query = urllib.parse.quote(final_query)
    return f"https://news.google.com/rss/search?q={encoded_query}&hl=ja&gl=JP&ceid=JP:ja"

def generate_news():
    api_key = os.environ.get("GEMINI_API_KEY")
    if not api_key:
        return {"column": "APIã‚­ãƒ¼ã‚¨ãƒ©ãƒ¼", "articles": [], "wordcloud": None}

    genai.configure(api_key=api_key)
    # å®‰å®šã—ã¦å‹•ä½œã™ã‚‹ gemini-2.5-flash-lite ã‚’æŒ‡å®š
    model = genai.GenerativeModel('gemini-2.5-flash-lite')

    rss_url = get_rss_url()
    print(f"ğŸ“° Googleãƒ‹ãƒ¥ãƒ¼ã‚¹ã‹ã‚‰è¨˜äº‹ã‚’å–å¾—ä¸­... (ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ•°: {len(KEYWORDS)})")
    
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"
    }
    
    try:
        response = requests.get(rss_url, headers=headers, timeout=10)
        feed = feedparser.parse(response.content)
    except Exception as e:
        return {"column": f"RSSå–å¾—ã‚¨ãƒ©ãƒ¼: {e}", "articles": [], "wordcloud": None}

    if not feed.entries:
        print("æŒ‡å®šã•ã‚ŒãŸæ¡ä»¶ï¼ˆ1æ—¥ä»¥å†…ï¼‰ã§è¨˜äº‹ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
        return {"column": "ç›´è¿‘24æ™‚é–“ã§ã®é–¢é€£ãƒ‹ãƒ¥ãƒ¼ã‚¹ã¯è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚", "articles": [], "wordcloud": None}

    articles = feed.entries[:MAX_ARTICLES]
    
    # AIã¸ã®æŒ‡ç¤º
    prompt = "ä»¥ä¸‹ã®ãƒ‹ãƒ¥ãƒ¼ã‚¹è¨˜äº‹ãƒªã‚¹ãƒˆã‚’èª­ã¿ã€Webã‚µã‚¤ãƒˆæ²è¼‰ç”¨ã®ãƒ‡ãƒ¼ã‚¿ã‚’JSONå½¢å¼ã§ä½œæˆã—ã¦ãã ã•ã„ã€‚\n"
    prompt += "ã€è¦ä»¶ã€‘\n"
    prompt += "1. `items`: å„è¨˜äº‹ã«ã¤ã„ã¦ã€catch_copy(30æ–‡å­—ä»¥å†…ã®è¦‹å‡ºã—)ã€ã¨ã€summary(100æ–‡å­—ç¨‹åº¦ã®è¦ç´„)ã€ã‚’ä½œæˆã€‚\n"
    prompt += "2. `column`: è¨˜äº‹å…¨ä½“ã‹ã‚‰èª­ã¿å–ã‚Œã‚‹ã€ä»Šæ—¥ã®AIãƒ»ãƒ†ãƒƒã‚¯æ¥­ç•Œã®å‹•ãã€ã‚’300æ–‡å­—ç¨‹åº¦ã®ã‚³ãƒ©ãƒ ã¨ã—ã¦ä½œæˆã€‚\n\n"
    prompt += "ã€è¨˜äº‹ãƒªã‚¹ãƒˆã€‘\n"
    
    for i, entry in enumerate(articles):
        prompt += f"ID:{i} ã‚¿ã‚¤ãƒˆãƒ«:{entry.title}\n"

    ai_data = {}
    try:
        response = model.generate_content(prompt)
        text = response.text

        # JSONæ•´å½¢å‡¦ç†
        if "```json" in text:
            text = text.replace("```json", "").replace("```", "")
        elif "```" in text:
            text = text.replace("```", "")
        
        ai_data = json.loads(text)
        
    except Exception as e:
        print(f"AIç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}")
        ai_data = {"column": f"AIç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}", "items": []}

    final_articles = []
    ai_items = ai_data.get("items", [])
    
    # ãƒ¯ãƒ¼ãƒ‰ã‚¯ãƒ©ã‚¦ãƒ‰ç”Ÿæˆç”¨ã®ãƒ†ã‚­ã‚¹ãƒˆãƒªã‚¹ãƒˆ
    text_for_wordcloud = []
    
    for i, entry in enumerate(articles):
        ai_item = ai_items[i] if i < len(ai_items) else {"catch_copy": entry.title, "summary": "è¦ç´„ç”Ÿæˆå¤±æ•—"}
        
        headline = ai_item.get("catch_copy", entry.title)
        summary = ai_item.get("summary", "")

        final_articles.append({
            "title": entry.title,
            "url": entry.link,
            "date": entry.published if 'published' in entry else "",
            "headline": headline,
            "summary": summary
        })
        
        # ã‚¿ã‚¤ãƒˆãƒ«ã€è¦‹å‡ºã—ã€è¦ç´„ã‚’ãƒ¯ãƒ¼ãƒ‰ã‚¯ãƒ©ã‚¦ãƒ‰ã®ç´ æã«è¿½åŠ 
        text_for_wordcloud.append(entry.title)
        text_for_wordcloud.append(headline)
        text_for_wordcloud.append(summary)

    # ãƒ¯ãƒ¼ãƒ‰ã‚¯ãƒ©ã‚¦ãƒ‰ç”»åƒã®ç”Ÿæˆã‚’å®Ÿè¡Œ
    wc_image_file = create_wordcloud(text_for_wordcloud)

    return {
        "column": ai_data.get("column", "ã‚³ãƒ©ãƒ ç”Ÿæˆå¤±æ•—"),
        "articles": final_articles,
        "wordcloud": wc_image_file # ç”Ÿæˆã•ã‚ŒãŸç”»åƒãƒ•ã‚¡ã‚¤ãƒ«åã‚’è¿½åŠ 
    }

if __name__ == "__main__":
    result = generate_news()
    print(f"URL: {get_rss_url()}")
    print(f"ã‚³ãƒ©ãƒ : {result['column'][:50]}...")
    print(f"è¨˜äº‹æ•°: {len(result['articles'])}")
    print(f"ãƒ¯ãƒ¼ãƒ‰ã‚¯ãƒ©ã‚¦ãƒ‰ç”»åƒ: {result['wordcloud']}")
