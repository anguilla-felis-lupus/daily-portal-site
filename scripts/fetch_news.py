import feedparser
import google.generativeai as genai
import os
import json

# --- è¨­å®š ---
# æ¤œç´¢ã—ãŸã„ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ï¼ˆã“ã“ã‚’å¤‰ãˆã‚Œã°åˆ¥ã®ãƒ‹ãƒ¥ãƒ¼ã‚¹ã«ãªã‚Šã¾ã™ï¼‰
RSS_URL = "https://news.google.com/rss/search?q=AI+Artificial+Intelligence+when:1d&hl=ja&gl=JP&ceid=JP:ja"
# å–å¾—ã™ã‚‹è¨˜äº‹æ•°
MAX_ARTICLES = 5

def generate_news():
    # 1. APIã‚­ãƒ¼ã®æº–å‚™
    api_key = os.environ.get("GEMINI_API_KEY")
    # ã‚¨ãƒ©ãƒ¼æ™‚ã¯è¾æ›¸å½¢å¼ã§ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’è¿”ã™
    if not api_key:
        return {"summary": "APIã‚­ãƒ¼ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚", "articles": []}

    genai.configure(api_key=api_key)
    # JSONå½¢å¼ã§
    model = genai.GenerativeModel(
        'gemini-1.5-flash',
        generation_config={"response_mime_type": "application/json"}
    )

    # 2. RSSã‹ã‚‰ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’å–å¾—
    print("ğŸ“° Googleãƒ‹ãƒ¥ãƒ¼ã‚¹ã‹ã‚‰è¨˜äº‹ã‚’å–å¾—ä¸­...")
    feed = feedparser.parse(RSS_URL)
    
    if not feed.entries:
        return {"column": "è¨˜äº‹ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚", "articles": []}

    # 3. è¨˜äº‹ãƒ‡ãƒ¼ã‚¿ã‚’æ•´å½¢
    raw_articles = feed.entries[:MAX_ARTICLES]
    
    print(f"âœ… {len(articles)}ä»¶ã®è¨˜äº‹ã‚’å–å¾—ã—ã¾ã—ãŸã€‚AIã«è¦ç´„ã‚’ä¾é ¼ã—ã¾ã™...")

    # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆä½œæˆ
    prompt = "ä»¥ä¸‹ã®ãƒ‹ãƒ¥ãƒ¼ã‚¹è¨˜äº‹ãƒªã‚¹ãƒˆã‚’èª­ã¿ã€Webã‚µã‚¤ãƒˆæ²è¼‰ç”¨ã®ãƒ‡ãƒ¼ã‚¿ã‚’JSONå½¢å¼ã§ä½œæˆã—ã¦ãã ã•ã„ã€‚\n"
    prompt += "ã€è¦ä»¶ã€‘\n"
    prompt += "1. `items`: å„è¨˜äº‹ã«ã¤ã„ã¦ã€catch_copy(30æ–‡å­—ä»¥å†…ã®è¦‹å‡ºã—)ã€ã¨ã€summary(100æ–‡å­—ç¨‹åº¦ã®è¦ç´„)ã€ã‚’ä½œæˆã€‚\n"
    prompt += "2. `column`: è¨˜äº‹å…¨ä½“ã‹ã‚‰èª­ã¿å–ã‚Œã‚‹ã€ä»Šæ—¥ã®AIæ¥­ç•Œã®å‹•ãã€ã‚’300æ–‡å­—ç¨‹åº¦ã®ã‚³ãƒ©ãƒ ã¨ã—ã¦ä½œæˆã€‚\n\n"
    prompt += "ã€è¨˜äº‹ãƒªã‚¹ãƒˆã€‘\n"

    for i, entry in enumerate(raw_articles):
        prompt += f"ID:{i} ã‚¿ã‚¤ãƒˆãƒ«:{entry.title}\n"

    try:
        # AIã«ç”Ÿæˆã•ã›ã‚‹
        response = model.generate_content(prompt)
        
        # JSONãƒ†ã‚­ã‚¹ãƒˆã‚’Pythonã®è¾æ›¸ãƒ‡ãƒ¼ã‚¿ã«å¤‰æ›
        ai_data = json.loads(response.text)
        
        # RSSã®å…ƒãƒ‡ãƒ¼ã‚¿(URLãªã©)ã¨ã€AIã®ç”Ÿæˆãƒ‡ãƒ¼ã‚¿(è¦ç´„)ã‚’åˆä½“ã•ã›ã‚‹
        final_articles = []
        ai_items = ai_data.get("items", [])
        
        for i, entry in enumerate(raw_articles):
            # AIã®ç”Ÿæˆãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Œã°ãã‚Œã‚’ä½¿ã†ã€ãªã‘ã‚Œã°ç©ºæ–‡å­—
            ai_item = ai_items[i] if i < len(ai_items) else {"catch_copy": entry.title, "summary": "è¦ç´„ç”Ÿæˆå¤±æ•—"}
            
            final_articles.append({
                "title": entry.title,
                "url": entry.link,
                "date": entry.published if 'published' in entry else "",
                "headline": ai_item.get("catch_copy", entry.title), # AIè¦‹å‡ºã—
                "summary": ai_item.get("summary", "")               # AIè¦ç´„
            })

        return {
            "column": ai_data.get("column", "ã‚³ãƒ©ãƒ ç”Ÿæˆå¤±æ•—"),
            "articles": final_articles
        }

    except Exception as e:
        print(f"ã‚¨ãƒ©ãƒ¼ç™ºç”Ÿ: {e}")
        return {"column": f"ã‚¨ãƒ©ãƒ¼: {e}", "articles": []}

if __name__ == "__main__":
    result = generate_news()
    print(f"ã‚³ãƒ©ãƒ : {result['column'][:50]}...")
    print(f"è¨˜äº‹æ•°: {len(result['articles'])}")
